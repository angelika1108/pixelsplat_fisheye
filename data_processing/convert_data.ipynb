{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shutil\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import yaml\n",
    "import re\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torchvision.transforms as tf\n",
    "import io\n",
    "from einops import rearrange, repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = \"02\"\n",
    "first_frame = 0\n",
    "last_frame = 249\n",
    "nb_frames = last_frame - first_frame + 1\n",
    "target_im_folder = f\"test_images_cam{cam}_{first_frame}_{last_frame}\"\n",
    "h_new = 360\n",
    "w_new = 640\n",
    "h_resize = 800\n",
    "w_resize = 800\n",
    "\n",
    "source_folder = f\"/home/angelika/pixelsplat_fisheye/datasets/kitti360/2013_05_28_drive_0000_sync/image_{cam}/data_rgb\"\n",
    "target_folder = f\"/home/angelika/pixelsplat_fisheye/datasets/kitti360/{target_im_folder}\"\n",
    "\n",
    "if not os.path.exists(target_folder):\n",
    "    os.makedirs(target_folder)\n",
    "\n",
    "\n",
    "# Get the list of filenames in the source folder\n",
    "filenames = sorted(os.listdir(source_folder))\n",
    "available_frames = []\n",
    "\n",
    "# Iterate over the filenames and copy images to the destination folder\n",
    "for i, filename in enumerate(filenames):\n",
    "    if i < nb_frames:\n",
    "        available_frames.append(i)\n",
    "        source_path = os.path.join(source_folder, filename)\n",
    "        filename_dest = filename\n",
    "        destination_path = os.path.join(target_folder, filename_dest)\n",
    "        \n",
    "        #shutil.copy2(source_path, destination_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 16)\n"
     ]
    }
   ],
   "source": [
    "# Copy camera poses\n",
    "pose_file = f\"/home/angelika/pixelsplat_fisheye/datasets/kitti360/2013_05_28_drive_0000_sync/all_cam{cam[1:]}_to_world.txt\"\n",
    "\n",
    "# Extract poses\n",
    "cam2world = []\n",
    "\n",
    "# Read cam2world matrices\n",
    "for line in open(pose_file, 'r').readlines():\n",
    "    value = list(map(float, line.strip().split(\" \")))\n",
    "    cam2world.append(np.array(value[1:]))\n",
    "\n",
    "cam2world = np.array(cam2world)\n",
    "cam2world = cam2world[first_frame:last_frame+1]\n",
    "print(cam2world.shape)\n",
    "# Save only the poses to a .txt file\n",
    "with open(f'{target_folder}/camera_poses.txt', 'w') as file:\n",
    "    for i, row in enumerate(cam2world):\n",
    "        # line = [filenames[available_frames[i]]] + [str(x) for x in row]\n",
    "        line = [str(available_frames[i])] + [str(x) for x in row]\n",
    "        line = ' '.join(line) + '\\n'\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict = {}\n",
    "\n",
    "image_dict['url'] = \"\"\n",
    "image_dict['key'] = f\"kitti360_cam{cam}_{first_frame}_{last_frame}\"\n",
    "\n",
    "timestamps = torch.as_tensor(available_frames)\n",
    "image_dict['timestamps'] = timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam2world12 = torch.tensor(cam2world[:, :12])\n",
    "two_zeros = torch.zeros((nb_frames, 2))\n",
    "intrinsics = torch.zeros((nb_frames, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the world2cam matrix from cam2world12\n",
    "# similary as in pixelsplat_data.ipynb.\n",
    "### Code in pixelsplat_data.ipynb:\n",
    "### w2c = repeat(torch.eye(4, dtype=torch.float32), \"h w -> b h w\", b=b).clone()   # shape: (256, 4, 4)\n",
    "### w2c[:, :3] = rearrange(poses[:, 6:], \"b (h w) -> b h w\", h=3, w=4)  # shape: (256, 3, 4)\n",
    "### extrinsics =  w2c.inverse()\n",
    "\n",
    "w2c12 = repeat(torch.eye(4, dtype=torch.float32), \"h w -> b h w\", b=nb_frames).clone()\n",
    "w2c12[:, :3] = rearrange(cam2world12, \"nb_frames (h w) -> nb_frames h w\", h=3, w=4)\n",
    "world2cam123 = w2c12.inverse()\n",
    "world2cam16 = world2cam123.reshape(nb_frames, 16)\n",
    "world2cam12 = world2cam16[:, :12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readYAMLFile(fileName):\n",
    "    '''make OpenCV YAML file compatible with python'''\n",
    "    ret = {}\n",
    "    skip_lines=1    # Skip the first line which says \"%YAML:1.0\". Or replace it with \"%YAML 1.0\"\n",
    "    with open(fileName) as fin:\n",
    "        for i in range(skip_lines):\n",
    "            fin.readline()\n",
    "        yamlFileOut = fin.read()\n",
    "        myRe = re.compile(r\":([^ ])\")   # Add space after \":\", if it doesn't exist. Python yaml requirement\n",
    "        yamlFileOut = myRe.sub(r': \\1', yamlFileOut)\n",
    "        ret = yaml.safe_load(yamlFileOut)\n",
    "    return ret\n",
    "\n",
    "intrinsics_file = f'/home/angelika/datasets/kitti_360/calibration/image_{cam}.yaml'\n",
    "intrinsics_dict = readYAMLFile(intrinsics_file)\n",
    "\n",
    "h_old = intrinsics_dict['image_height']\n",
    "w_old = intrinsics_dict['image_width']\n",
    "\n",
    "intrinsics[:, 0] = torch.tensor(intrinsics_dict['projection_parameters']['gamma1']).repeat(nb_frames) / w_old\n",
    "intrinsics[:, 1] = torch.tensor(intrinsics_dict['projection_parameters']['gamma2']).repeat(nb_frames) / h_old\n",
    "intrinsics[:, 2] = torch.tensor(intrinsics_dict['projection_parameters']['u0']).repeat(nb_frames) / w_old\n",
    "intrinsics[:, 3] = torch.tensor(intrinsics_dict['projection_parameters']['v0']).repeat(nb_frames) / h_old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera parameters\n",
    "poses = torch.cat((intrinsics, two_zeros, world2cam12), dim=1)\n",
    "poses.shape\n",
    "\n",
    "image_dict['cameras'] = poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert images and save them to image_dict\n",
    "\n",
    "flattened_images = []\n",
    "\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "    if i < nb_frames:\n",
    "        available_frames.append(i)\n",
    "        source_path = os.path.join(source_folder, filename)\n",
    "        filename_dest = filename\n",
    "        destination_path = os.path.join(target_folder, filename_dest)\n",
    "        image = Image.open(source_path)\n",
    "        im_resize = image.resize((w_resize, h_resize))\n",
    "        im_crop = im_resize.crop((80, 220, 720, 580))\n",
    "        #im = Image.new(im_resize.mode, (w_new, h_new), (0,0,0))\n",
    "        #im.paste(im_resize, ((w_new-360)//2, (h_new-360)//2))\n",
    "        \n",
    "        # Convert image to bytes\n",
    "        byte_stream = BytesIO()\n",
    "        im_crop.save(byte_stream, format='PNG')  # Choose appropriate format, e.g., JPEG, PNG, etc.\n",
    "        image_bytes = byte_stream.getvalue()\n",
    "        byte_stream.close()\n",
    "\n",
    "        # Convert bytes to tensor\n",
    "        frameTensor = torch.tensor(np.frombuffer(image_bytes, dtype=np.uint8))\n",
    "        \n",
    "        flattened_images.append(frameTensor)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_rows = 5  \n",
    "num_cols = nb_frames // num_rows  \n",
    "\n",
    "subplot_width = 8  \n",
    "subplot_height = 6 \n",
    "\n",
    "fig_width = subplot_width * num_cols\n",
    "fig_height = subplot_height * num_rows\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(fig_width, fig_height))\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "    if i < nb_frames:\n",
    "        available_frames.append(i)\n",
    "        filename_dest = filename\n",
    "        destination_path = os.path.join(target_folder, filename_dest)\n",
    "        source_path = os.path.join(source_folder, filename)\n",
    "        image = Image.open(source_path)\n",
    "        im_resize = image.resize((w_resize, h_resize))\n",
    "        im_crop = im_resize.crop((80, 220, 720, 580))\n",
    "        im_crop.save(destination_path)\n",
    "\n",
    "\n",
    "        # Calculate subplot indices\n",
    "        row_index = i // num_cols\n",
    "        col_index = i % num_cols\n",
    "\n",
    "        # Plot the image\n",
    "        axes[row_index, col_index].imshow(im_crop)\n",
    "        axes[row_index, col_index].axis('off')\n",
    "        axes[row_index, col_index].set_title(f\"Frame {i}\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test one image\n",
    "# byte_stream = BytesIO()\n",
    "# image.save(byte_stream, format='PNG')  # Choose appropriate format, e.g., JPEG, PNG, etc.\n",
    "# image_bytes = byte_stream.getvalue()\n",
    "# byte_stream.close()\n",
    "\n",
    "# frameTensor = torch.tensor(np.frombuffer(image_bytes, dtype=np.uint8))\n",
    "\n",
    "# Check if the image is loaded correctly\n",
    "# image2 = Image.open(BytesIO(frameTensor.numpy().tobytes()))\n",
    "# image2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the images\n",
    "image_dict['images'] = flattened_images\n",
    "\n",
    "# Save the image_dict to a .torch file\n",
    "data = [image_dict]\n",
    "\n",
    "# Save the image_dict to a .torch file\n",
    "#data = [image_dict, image_dict, image_dict, image_dict]\n",
    "\n",
    "#data[0]['key'] = f\"kitti360_cam{cam}_{first_frame}_{last_frame}_0\"\n",
    "#data[1]['key'] = f\"kitti360_cam{cam}_{first_frame}_{last_frame}_1\"\n",
    "#data[2]['key'] = f\"kitti360_cam{cam}_{first_frame}_{last_frame}_2\"\n",
    "#data[3]['key'] = f\"kitti360_cam{cam}_{first_frame}_{last_frame}_3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dict_name = \"000000\"\n",
    "torch.save(data, f'{target_folder}/{image_dict_name}.torch')\n",
    "torch.save(data, f'/home/angelika/pixelsplat_fisheye/datasets/kitti360/test/{image_dict_name}.torch')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fisheye_psplat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad6da4dfbd24789c621481ef99b6ca206c23abc5e438488209ed0f151c17c055"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
